import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
import os
import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from einops import rearrange
from einops.layers.torch import Rearrange
from mpl_toolkits.mplot3d import Axes3D
from skimage.transform import resize

class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):
        super().__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, 
                                  stride=stride, padding=padding, groups=in_channels)
        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)
    
    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x

class DHAMAttention(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.channels = channels
        
        self.spatial = nn.Sequential(
            nn.Conv2d(channels, 1, kernel_size=7, padding=3),
            nn.Sigmoid()
        )
        
        self.channel = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels // 16, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d(channels // 16, channels, kernel_size=1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        spatial_attn = self.spatial(x)
        channel_attn = self.channel(x)
        
        self.spatial_attn_map = spatial_attn.detach()
        self.channel_attn_map = channel_attn.detach()
        
        out = x * spatial_attn * channel_attn
        return out

class SimpleViT(nn.Module):
    def __init__(self, in_channels, patch_size, num_patches, dim, depth, heads):
        super().__init__()
        
        self.patch_embedding = nn.Sequential(
            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', 
                     p1=patch_size, p2=patch_size),
            nn.Linear(patch_size * patch_size * in_channels, dim)
        )
        
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=dim, nhead=heads), num_layers=depth
        )
        
        self.mlp_head = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, dim)
        )
        
    def forward(self, x):
        x = self.patch_embedding(x)
        x = self.transformer(x)
        x = x.mean(dim=1)
        x = self.mlp_head(x)
        return x

class CompleteModel(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        
        self.features = nn.Sequential(
            DepthwiseSeparableConv(3, 32, 3),
            nn.BatchNorm2d(32),  
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            DepthwiseSeparableConv(32, 64, 3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            DepthwiseSeparableConv(64, 128, 3),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        
        self.DHAM = DHAMAttention(128)
        self.vit = SimpleViT(
            in_channels=128,
            patch_size=2,  
            num_patches=16,
            dim=128,      
            depth=1,      
            heads=4
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),  # 降低dropout率
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
        
    def forward(self, x):
        x = self.features(x)
        x = self.DHAM(x)
        x = self.vit(x)
        x = self.classifier(x)
        return x

def visualize_attention(model, image_batch, save_path):
    save_dir = os.path.dirname(save_path)
    base_name = os.path.splitext(os.path.basename(save_path))[0]
    attention_save_path = os.path.join(save_dir, f'{base_name}_attention.png')
    
    plt.figure(figsize=(15, 5))
    with torch.no_grad():
        _ = model(image_batch)
    spatial_attn = model.DHAM.spatial_attn_map[0].cpu().numpy()
    channel_attn = model.DHAM.channel_attn_map[0].cpu().numpy()
    plt.subplot(131)
    img = image_batch[0].cpu().numpy()
    mean = np.array([0.485, 0.456, 0.406]).reshape(-1, 1, 1)
    std = np.array([0.229, 0.224, 0.225]).reshape(-1, 1, 1)
    img = std * img + mean
    img = np.clip(img.transpose(1, 2, 0), 0, 1)
    plt.imshow(img)
    plt.title('Original Image')
    plt.axis('off')
    plt.subplot(132)
    plt.imshow(spatial_attn[0], cmap='jet')
    plt.title('Spatial Attention')
    plt.axis('off')

    plt.subplot(133)
    channel_attn = channel_attn.squeeze()
    channel_attn = channel_attn.reshape(1, -1)
    sns.heatmap(channel_attn, cmap='jet', xticklabels=False, yticklabels=False)
    plt.title('Channel Attention')
    
    plt.tight_layout()
    plt.savefig(attention_save_path)
    plt.close()
    
    feature_save_path = os.path.join(save_dir, f'{base_name}_3d_features.png')
    visualize_3d_features(model, image_batch, feature_save_path)
    
    paper_feature_save_path = os.path.join(save_dir, f'{base_name}_paper_3d_features.png')
    visualize_3d_features_for_paper(model, image_batch, paper_feature_save_path)

def visualize_3d_features(model, image_batch, save_path):
    features = model.features(image_batch)  
    feature_map = features[0].detach().cpu().numpy()  
    
    fig = plt.figure(figsize=(15, 7))
    
    ax1 = fig.add_subplot(121, projection='3d')
    
    C, H, W = feature_map.shape
    
    x, y = np.meshgrid(np.arange(W), np.arange(H))
    
    channel_step = 5
    selected_channels = range(0, C, channel_step)
    
    colors = plt.cm.jet(np.linspace(0, 1, len(selected_channels)))
    
    for idx, channel in enumerate(selected_channels):
        z = feature_map[channel]
        
        z = (z - z.min()) / (z.max() - z.min() + 1e-8)
        
        surf = ax1.plot_surface(x, y, z, alpha=0.3, color=colors[idx])
    
    ax1.set_xlabel('Width')
    ax1.set_ylabel('Height')
    ax1.set_zlabel('Feature Intensity')
    ax1.set_title('3D Feature Visualization')
    
    ax2 = fig.add_subplot(122)
    
    mean_activation = np.mean(feature_map, axis=0)
    
    im = ax2.imshow(mean_activation, cmap='jet')
    plt.colorbar(im, ax=ax2)
    ax2.set_title('Average Feature Activation')
    
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()

def visualize_3d_features_for_paper(model, image_batch, save_path):
    with torch.no_grad():
        features = model.features(image_batch)
    feature_map = features[0].detach().cpu().numpy()
    
    fig = plt.figure(figsize=(12, 10))
    ax = fig.add_subplot(111, projection='3d')
    
    C, H, W = feature_map.shape
    
    try:
        img = image_batch[0].cpu().numpy()
        mean = np.array([0.485, 0.456, 0.406]).reshape(-1, 1, 1)
        std = np.array([0.229, 0.224, 0.225]).reshape(-1, 1, 1)
        img = std * img + mean
        img = np.clip(img.transpose(1, 2, 0), 0, 1)
        
        x = np.linspace(0, W-1, W)
        y = np.linspace(0, H-1, H)
        X, Y = np.meshgrid(x, y)
        
        img_resized = resize(img, (H, W, 3))
        img_resized = np.clip(img_resized, 0, 1)
        
        ax.plot_surface(X, Y, np.zeros_like(X), 
                       facecolors=img_resized,
                       shade=False,
                       zorder=1)
        
        channel_step = 5
        selected_channels = range(0, C, channel_step)
        colors = plt.cm.rainbow(np.linspace(0, 1, len(selected_channels)))
        
        ax.view_init(elev=20, azim=45)
        
        z_scale = 1.5
        for idx, channel in enumerate(selected_channels):
            z = feature_map[channel]
            z = (z - z.min()) / (z.max() - z.min() + 1e-8)
            surf = ax.plot_surface(X, Y, z * z_scale, 
                                 alpha=0.3,
                                 color=colors[idx],
                                 zorder=2)
        
        ax.set_xlim(0, W-1)
        ax.set_ylim(0, H-1)
        ax.set_zlim(0, z_scale)
        
        ax.set_axis_off()
        ax.grid(False)
        
        plt.tight_layout()
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        print(f"Saving 3D visualization to: {save_path}")
        plt.savefig(save_path, 
                   dpi=300, 
                   bbox_inches='tight', 
                   pad_inches=0,
                   transparent=True)
        print(f"Successfully saved 3D visualization")
        
    except Exception as e:
        print(f"Error in visualize_3d_features_for_paper: {e}")
        print(f"Current working directory: {os.getcwd()}")
        if 'img_resized' in locals():
            print(f"Image range: [{img_resized.min():.3f}, {img_resized.max():.3f}]")
    finally:
        plt.close()

def main():
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    result_dir = os.path.join('result', timestamp)
    os.makedirs(result_dir, exist_ok=True)
    
    transform_train = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                            std=[0.229, 0.224, 0.225])
    ])
    
    transform_test = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                            std=[0.229, 0.224, 0.225])
    ])
    
    train_dataset = ImageFolder('data/train', transform=transform_train)
    test_dataset = ImageFolder('data/test', transform=transform_test)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = CompleteModel().to(device)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        mode='min',
        factor=0.5,
        patience=3,    
        verbose=True,
        min_lr=1e-6    
    )
    
    num_epochs = 30    
    
    best_val_loss = float('inf')
    patience = 25       
    patience_counter = 0 
    vis_frequency = 50  
    

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        for i, (images, labels) in enumerate(train_loader):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            if i % 10 == 0:
                print(f'Epoch [{epoch+1}/{num_epochs}], '
                      f'Step [{i+1}/{len(train_loader)}], '
                      f'Loss: {loss.item():.4f}')
            
            if i % vis_frequency == 0:
                try:
                    print(f"\nGenerating visualization for epoch {epoch}, batch {i}")
                    vis_path = os.path.join(result_dir, f'visualization_epoch{epoch}_batch{i}')
                    print(f"Visualization path: {vis_path}")
                    
                    model = model.cpu()
                    images_cpu = images.cpu()
                    
                    visualize_attention(model, images_cpu, vis_path)
                    print("Visualization completed successfully")
                    
                    model = model.to(device)
                except Exception as e:
                    print(f"Visualization error: {e}")
                    print(f"Current working directory: {os.getcwd()}")
        
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
            try:
                test_images, _ = next(iter(test_loader))
                vis_path = os.path.join(result_dir, f'visualization_epoch{epoch}_validation')
                visualize_attention(model.cpu(), test_images.cpu(), vis_path)
                model.to(device)
            except Exception as e:
                print(f"Validation visualization error: {e}")
        
        train_loss = train_loss / len(train_loader)
        val_loss = val_loss / len(test_loader)
        accuracy = 100. * correct / total
        
        print(f'Epoch [{epoch+1}/{num_epochs}]')
        print(f'Train Loss: {train_loss:.4f}')
        print(f'Val Loss: {val_loss:.4f}')
        print(f'Accuracy: {accuracy:.2f}%')
        
        scheduler.step(val_loss)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), 
                      os.path.join(result_dir, 'best_model.pth'))
            
            try:
                vis_path = os.path.join(result_dir, f'visualization_best_model_epoch{epoch}')
                visualize_attention(model.cpu(), test_images.cpu(), vis_path)
                model.to(device)
            except Exception as e:
                print(f"Best model visualization error: {e}")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("Early stopping triggered")
                break
    
    torch.save(model.state_dict(), os.path.join(result_dir, 'final_model.pth'))
    
    try:
        vis_path = os.path.join(result_dir, 'visualization_final')
        visualize_attention(model.cpu(), test_images.cpu(), vis_path)
    except Exception as e:
        print(f"Final visualization error: {e}")

if __name__ == '__main__':
    main() 
